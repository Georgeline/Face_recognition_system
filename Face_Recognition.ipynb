{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1g3ot4N8XpjcGR2FBUtyWOAHyaKYFFGMC","authorship_tag":"ABX9TyOrks6kxxHCjPOWC+s/kcZ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip uninstall opencv-python\n","!pip install opencv-python opencv-contrib-python"],"metadata":{"id":"5gNj_CJ_6AmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCmTAyDU6uSa","executionInfo":{"status":"ok","timestamp":1739712247785,"user_tz":-60,"elapsed":899,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"e6fc8bd3-776b-4c8b-dd28-eb20ec00d1b5"},"outputs":[{"output_type":"stream","name":"stdout","text":[" abass.jpg\t\t   IMG-20250208-WA0012.jpg   IMG-20250208-WA0037.jpg\n"," Abdullah.jpg\t\t   IMG-20250208-WA0013.jpg   IMG-20250208-WA0038.jpg\n"," Ashraff.jpg\t\t   IMG-20250208-WA0014.jpg   IMG-20250208-WA0039.jpg\n"," donatus.jpg\t\t   IMG-20250208-WA0015.jpg   IMG-20250208-WA0040.jpg\n"," drino.jpg\t\t   IMG-20250208-WA0016.jpg   IMG-20250208-WA0041.jpg\n"," fagbulu.jpg\t\t   IMG-20250208-WA0017.jpg   IMG-20250208-WA0042.jpg\n","'fidelix 2.jpg'\t\t   IMG-20250208-WA0018.jpg   IMG-20250208-WA0042.zip\n"," fidelix.jpg\t\t   IMG-20250208-WA0019.jpg   jakusi.jpg\n","'imam Basheer.jpg'\t   IMG-20250208-WA0020.jpg   jamiu.jpg\n"," IMG-20241021-WA0028.jpg   IMG-20250208-WA0021.jpg   mickey.jpg\n"," IMG-20250207-WA0021.jpg   IMG-20250208-WA0022.jpg   mubarak.jpg\n"," IMG-20250208-WA0003.jpg   IMG-20250208-WA0023.jpg   nurse.jpg\n"," IMG-20250208-WA0004.jpg   IMG-20250208-WA0024.jpg   okiki.jpg\n"," IMG-20250208-WA0005.jpg   IMG-20250208-WA0025.jpg   olacash.jpg\n"," IMG-20250208-WA0006.jpg   IMG-20250208-WA0026.jpg  'omo pastor.jpg'\n"," IMG-20250208-WA0007.jpg   IMG-20250208-WA0032.jpg   Timon.jpg\n"," IMG-20250208-WA0008.jpg   IMG-20250208-WA0033.jpg   tory.jpg\n"," IMG-20250208-WA0009.jpg   IMG-20250208-WA0034.jpg   unique.jpg\n"," IMG-20250208-WA0010.jpg   IMG-20250208-WA0035.jpg\n"," IMG-20250208-WA0011.jpg   IMG-20250208-WA0036.jpg\n"]}],"source":["# Intended to access a file, not call a function\n","!ls \"//content/drive/MyDrive/face_recognition_db\""]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/face_recognition_db"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTJsbG3W8hFk","executionInfo":{"status":"ok","timestamp":1739712248785,"user_tz":-60,"elapsed":45,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"c1613729-7c30-4b45-a496-6328cb2445c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" abass.jpg\t\t   IMG-20250208-WA0012.jpg   IMG-20250208-WA0037.jpg\n"," Abdullah.jpg\t\t   IMG-20250208-WA0013.jpg   IMG-20250208-WA0038.jpg\n"," Ashraff.jpg\t\t   IMG-20250208-WA0014.jpg   IMG-20250208-WA0039.jpg\n"," donatus.jpg\t\t   IMG-20250208-WA0015.jpg   IMG-20250208-WA0040.jpg\n"," drino.jpg\t\t   IMG-20250208-WA0016.jpg   IMG-20250208-WA0041.jpg\n"," fagbulu.jpg\t\t   IMG-20250208-WA0017.jpg   IMG-20250208-WA0042.jpg\n","'fidelix 2.jpg'\t\t   IMG-20250208-WA0018.jpg   IMG-20250208-WA0042.zip\n"," fidelix.jpg\t\t   IMG-20250208-WA0019.jpg   jakusi.jpg\n","'imam Basheer.jpg'\t   IMG-20250208-WA0020.jpg   jamiu.jpg\n"," IMG-20241021-WA0028.jpg   IMG-20250208-WA0021.jpg   mickey.jpg\n"," IMG-20250207-WA0021.jpg   IMG-20250208-WA0022.jpg   mubarak.jpg\n"," IMG-20250208-WA0003.jpg   IMG-20250208-WA0023.jpg   nurse.jpg\n"," IMG-20250208-WA0004.jpg   IMG-20250208-WA0024.jpg   okiki.jpg\n"," IMG-20250208-WA0005.jpg   IMG-20250208-WA0025.jpg   olacash.jpg\n"," IMG-20250208-WA0006.jpg   IMG-20250208-WA0026.jpg  'omo pastor.jpg'\n"," IMG-20250208-WA0007.jpg   IMG-20250208-WA0032.jpg   Timon.jpg\n"," IMG-20250208-WA0008.jpg   IMG-20250208-WA0033.jpg   tory.jpg\n"," IMG-20250208-WA0009.jpg   IMG-20250208-WA0034.jpg   unique.jpg\n"," IMG-20250208-WA0010.jpg   IMG-20250208-WA0035.jpg\n"," IMG-20250208-WA0011.jpg   IMG-20250208-WA0036.jpg\n"]}]},{"cell_type":"code","source":["pip install tensorflow keras opencv-python dlib numpy scikit-learn matplotlib\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"waloG1uJFwxM","executionInfo":{"status":"ok","timestamp":1739712256717,"user_tz":-60,"elapsed":7948,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"07db3553-5080-4b8e-e8d2-2f1db239a306"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: dlib in /usr/local/lib/python3.11/dist-packages (19.24.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"]}]},{"cell_type":"code","source":["pip install mtcnn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07k8UTrVF9YW","executionInfo":{"status":"ok","timestamp":1739712261582,"user_tz":-60,"elapsed":4893,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"56bbbaba-ee3f-4106-a5f8-390ca0c4535c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mtcnn in /usr/local/lib/python3.11/dist-packages (1.0.0)\n","Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn) (1.4.2)\n","Requirement already satisfied: lz4>=4.3.3 in /usr/local/lib/python3.11/dist-packages (from mtcnn) (4.4.3)\n"]}]},{"cell_type":"code","source":["import os # Import the os module\n","\n","# Define DATASET_PATH before using it\n","DATASET_PATH = \"/content/drive/MyDrive/face_recognition_db\"\n","\n","\n","files = os.listdir(DATASET_PATH)\n","print(\"Files in dataset:\", files)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7tVZpoP8936I","executionInfo":{"status":"ok","timestamp":1739712261587,"user_tz":-60,"elapsed":201,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"f3cc60e0-9c65-406a-9877-12e0a8b05eb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files in dataset: ['IMG-20250208-WA0042.zip', 'abass.jpg', 'Abdullah.jpg', 'Ashraff.jpg', 'donatus.jpg', 'drino.jpg', 'fagbulu.jpg', 'fidelix 2.jpg', 'fidelix.jpg', 'imam Basheer.jpg', 'IMG-20241021-WA0028.jpg', 'IMG-20250207-WA0021.jpg', 'IMG-20250208-WA0003.jpg', 'IMG-20250208-WA0004.jpg', 'IMG-20250208-WA0005.jpg', 'IMG-20250208-WA0006.jpg', 'IMG-20250208-WA0007.jpg', 'IMG-20250208-WA0008.jpg', 'IMG-20250208-WA0009.jpg', 'IMG-20250208-WA0010.jpg', 'IMG-20250208-WA0011.jpg', 'IMG-20250208-WA0012.jpg', 'IMG-20250208-WA0013.jpg', 'IMG-20250208-WA0014.jpg', 'IMG-20250208-WA0015.jpg', 'IMG-20250208-WA0016.jpg', 'IMG-20250208-WA0017.jpg', 'IMG-20250208-WA0018.jpg', 'IMG-20250208-WA0019.jpg', 'IMG-20250208-WA0020.jpg', 'IMG-20250208-WA0021.jpg', 'IMG-20250208-WA0022.jpg', 'IMG-20250208-WA0023.jpg', 'IMG-20250208-WA0024.jpg', 'IMG-20250208-WA0025.jpg', 'IMG-20250208-WA0026.jpg', 'IMG-20250208-WA0032.jpg', 'IMG-20250208-WA0033.jpg', 'IMG-20250208-WA0034.jpg', 'IMG-20250208-WA0035.jpg', 'IMG-20250208-WA0036.jpg', 'IMG-20250208-WA0037.jpg', 'IMG-20250208-WA0038.jpg', 'IMG-20250208-WA0039.jpg', 'IMG-20250208-WA0040.jpg', 'IMG-20250208-WA0041.jpg', 'IMG-20250208-WA0042.jpg', 'jakusi.jpg', 'jamiu.jpg', 'mickey.jpg', 'mubarak.jpg', 'nurse.jpg', 'okiki.jpg', 'olacash.jpg', 'omo pastor.jpg', 'Timon.jpg', 'tory.jpg', 'unique.jpg']\n"]}]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","DATASET_PATH = \"/content/drive/MyDrive/face_recognition_db\"\n","ZIP_PATH = os.path.join(DATASET_PATH, \"IMG-20250208-WA0042.zip\")\n","\n","# Extract ZIP if it exists\n","if os.path.exists(ZIP_PATH):\n","    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n","        zip_ref.extractall(DATASET_PATH)  # Extract inside the same directory\n","    print(\"Extraction complete!\")\n","else:\n","    print(\"Error: ZIP file not found!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fdSR0pyA-KKb","executionInfo":{"status":"ok","timestamp":1739712261589,"user_tz":-60,"elapsed":189,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"979d7f0f-824f-4c1b-9d9f-1e47a09bec8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extraction complete!\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os\n","\n","DATASET_PATH = \"/content/drive/MyDrive/face_recognition_db\"\n","IMAGE_SIZE = (160, 160)  # FaceNet input size\n","\n","def load_images(directory):\n","    images = []\n","    labels = []\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n","            img_path = os.path.join(directory, filename)\n","            img = cv2.imread(img_path)\n","            img = cv2.resize(img, IMAGE_SIZE)  # Resize to 160x160\n","            img = img.astype(\"float32\") / 255.0  # Normalize\n","            images.append(img)\n","            labels.append(filename.split(\".\")[0])  # Use filename as label\n","    return np.array(images), np.array(labels)\n","\n","images, labels = load_images(DATASET_PATH)\n","print(f\"Loaded {len(images)} images\")\n","# Removed extra double quotes at the end of the first code block\n","#IMAGE_SIZE = (160, 160)  # FaceNet input size - this line was duplicated\n","#def load_images(directory): - this function was also duplicated\n","#    images = []\n","#    labels = []\n","#    for filename in os.listdir(directory):\n","#        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n","#            img_path = os.path.join(directory, filename)\n","#            img = cv2.imread(img_path)\n","#            img = cv2.resize(img, IMAGE_SIZE)  # Resize to 160x160\n","#            img = img.astype(\"float32\") / 255.0  # Normalize\n","#            images.append(img)\n","#            labels.append(filename.split(\".\")[0])  # Use filename as label\n","#    return np.array(images), np.array(labels)\n","#\n","#images, labels = load_images(DATASET_PATH)\n","#print(f\"Loaded {len(images)} images\")\n","def load_images(directory):\n","    images = []\n","    labels = []\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n","            img_path = os.path.join(directory, filename)\n","            img = cv2.imread(img_path)\n","            img = cv2.resize(img, IMAGE_SIZE)  # Resize to 160x160\n","            img = img.astype(\"float32\") / 255.0  # Normalize\n","            images.append(img)\n","            labels.append(filename.split(\".\")[0])  # Use filename as label\n","    return np.array(images), np.array(labels)\n","\n","images, labels = load_images(DATASET_PATH)\n","print(f\"Loaded {len(images)} images\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TF1Na8kg7P12","executionInfo":{"status":"ok","timestamp":1739712270146,"user_tz":-60,"elapsed":8641,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"4133255d-5cfb-4545-c968-faf00bcab9d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 57 images\n","Loaded 57 images\n"]}]},{"cell_type":"code","source":["from mtcnn import MTCNN\n","import cv2\n","import numpy as np\n","\n","# Initialize MTCNN detector\n","detector = MTCNN()\n","\n","def detect_faces(image):\n","    # Ensure image is in uint8 format (0-255)\n","    if image.dtype == np.float32:\n","        image = (image * 255).astype(np.uint8)\n","\n","    # Convert BGR (OpenCV format) to RGB (MTCNN format)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Detect faces\n","    faces = detector.detect_faces(image)\n","    return faces\n","\n","# Check images before detecting faces\n","for i, img in enumerate(images):\n","    if img is None or img.size == 0:\n","        print(f\"Skipping empty image {i}\")\n","        continue\n","\n","    faces = detect_faces(img)\n","    print(f\"Detected {len(faces)} faces in image {i}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTY0MCIj-sIh","executionInfo":{"status":"ok","timestamp":1739712298927,"user_tz":-60,"elapsed":28864,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"c0b89560-7aa2-4510-eefd-6d253c282af5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected 1 faces in image 0\n","Detected 1 faces in image 1\n","Detected 1 faces in image 2\n","Detected 1 faces in image 3\n","Detected 1 faces in image 4\n","Detected 1 faces in image 5\n","Detected 1 faces in image 6\n","Detected 1 faces in image 7\n","Detected 2 faces in image 8\n","Detected 1 faces in image 9\n","Detected 2 faces in image 10\n","Detected 1 faces in image 11\n","Detected 0 faces in image 12\n","Detected 1 faces in image 13\n","Detected 1 faces in image 14\n","Detected 1 faces in image 15\n","Detected 1 faces in image 16\n","Detected 1 faces in image 17\n","Detected 1 faces in image 18\n","Detected 1 faces in image 19\n","Detected 2 faces in image 20\n","Detected 1 faces in image 21\n","Detected 1 faces in image 22\n","Detected 1 faces in image 23\n","Detected 1 faces in image 24\n","Detected 1 faces in image 25\n","Detected 1 faces in image 26\n","Detected 1 faces in image 27\n","Detected 1 faces in image 28\n","Detected 1 faces in image 29\n","Detected 1 faces in image 30\n","Detected 1 faces in image 31\n","Detected 1 faces in image 32\n","Detected 1 faces in image 33\n","Detected 1 faces in image 34\n","Detected 1 faces in image 35\n","Detected 1 faces in image 36\n","Detected 1 faces in image 37\n","Detected 1 faces in image 38\n","Detected 1 faces in image 39\n","Detected 1 faces in image 40\n","Detected 1 faces in image 41\n","Detected 1 faces in image 42\n","Detected 1 faces in image 43\n","Detected 1 faces in image 44\n","Detected 1 faces in image 45\n","Detected 1 faces in image 46\n","Detected 1 faces in image 47\n","Detected 1 faces in image 48\n","Detected 1 faces in image 49\n","Detected 2 faces in image 50\n","Detected 1 faces in image 51\n","Detected 1 faces in image 52\n","Detected 2 faces in image 53\n","Detected 1 faces in image 54\n","Detected 1 faces in image 55\n","Detected 1 faces in image 56\n"]}]},{"cell_type":"code","source":["import os\n","\n","model_path = \"20180402-114759.pb\"\n","print(\"Current directory:\", os.getcwd())\n","print(\"Files in directory:\", os.listdir(os.getcwd()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmMgUqijHUAG","executionInfo":{"status":"ok","timestamp":1739712298934,"user_tz":-60,"elapsed":619,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"a2b4f2ca-44de-4e74-cf3f-0821fdc4409f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current directory: /content\n","Files in directory: ['.config', 'embeddings.json', 'drive', 'detected_faces', 'face_embeddings.json', 'sample_data']\n"]}]},{"cell_type":"code","source":["# Intended to access a file, not call a function\n","!ls \"/content/drive/MyDrive/facenet_model/20180408-102900/20180408-102900.pb\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOWhFzrXt-aQ","executionInfo":{"status":"ok","timestamp":1739712298936,"user_tz":-60,"elapsed":492,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"20b5fbe8-be22-46d5-f8a8-41784c1d07a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/facenet_model/20180408-102900/20180408-102900.pb\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Disable eager execution for TensorFlow 2.x\n","tf.compat.v1.disable_eager_execution()\n","\n","model_path = \"/content/drive/MyDrive/facenet_model/20180408-102900/20180408-102900.pb\"\n","\n","def load_facenet_model(model_path):\n","    with tf.io.gfile.GFile(model_path, \"rb\") as f:\n","        graph_def = tf.compat.v1.GraphDef()\n","        graph_def.ParseFromString(f.read())\n","\n","    # Create a new session before importing the model\n","    session = tf.compat.v1.Session()\n","    with session.as_default():\n","        tf.import_graph_def(graph_def, name=\"\")\n","\n","    return session\n","\n","# Load the FaceNet model\n","session = load_facenet_model(model_path)\n","\n","# Get input/output tensors for FaceNet\n","input_tensor = session.graph.get_tensor_by_name(\"input:0\")\n","embedding_tensor = session.graph.get_tensor_by_name(\"embeddings:0\")\n","phase_train_tensor = session.graph.get_tensor_by_name(\"phase_train:0\")\n","\n","print(\"FaceNet model loaded successfully!\")\n","\n"],"metadata":{"id":"PTZnduX2AKFW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739712298936,"user_tz":-60,"elapsed":393,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"18cc8e41-87af-4b0f-a0b2-54c015d1c8de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FaceNet model loaded successfully!\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","def get_face_embedding(session, image):\n","    # Ensure image is float32 and mean-centered (FaceNet preprocessing)\n","    image = image.astype('float32')\n","    mean, std = image.mean(), image.std()\n","    image = (image - mean) / std  # Standardize\n","\n","    # Expand dimensions to match FaceNet input shape\n","    face_pixels = np.expand_dims(image, axis=0)\n","\n","    # Get input and output tensors from the session graph\n","    graph = tf.compat.v1.get_default_graph()\n","    images_placeholder = graph.get_tensor_by_name(\"input:0\")  # Input tensor\n","    embeddings_tensor = graph.get_tensor_by_name(\"embeddings:0\")  # Output tensor\n","    phase_train_placeholder = graph.get_tensor_by_name(\"phase_train:0\")  # Training phase\n","\n","    # Run FaceNet model\n","    embedding = session.run(embeddings_tensor, feed_dict={\n","        images_placeholder: face_pixels,\n","        phase_train_placeholder: False  # Ensure inference mode\n","    })\n","\n","    print(\"Face Embedding Shape:\", embedding.shape)  # Print the shape\n","    print(\"Face Embedding Values:\", embedding)  # Print the actual values\n","\n","    return embedding\n","\n"],"metadata":{"id":"dx6xwSvduu4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import cv2\n","import numpy as np\n","\n","# Ensure compatibility mode if using TensorFlow 2.x\n","tf.compat.v1.disable_eager_execution()\n","\n","# Start a TensorFlow session\n","sess = tf.compat.v1.Session()\n","\n","# Load pre-trained FaceNet model (update path accordingly)\n","model_path = \"/content/drive/MyDrive/facenet_model/20180408-102900/20180408-102900.pb\"\n","\n","# Load the frozen graph (.pb file)\n","with tf.io.gfile.GFile(model_path, \"rb\") as f:\n","    graph_def = tf.compat.v1.GraphDef()\n","    graph_def.ParseFromString(f.read())\n","\n","with sess.as_default():\n","    tf.import_graph_def(graph_def, name=\"\")\n","\n","# Define a function for face embedding extraction\n","def get_face_embedding(sess, image):\n","    # Ensure 'image' has the correct shape (1, 160, 160, 3)\n","    image = np.expand_dims(image, axis=0)\n","\n","    # Get input and output tensors\n","    input_tensor = sess.graph.get_tensor_by_name(\"input:0\")  # Ensure correct name\n","    embedding_tensor = sess.graph.get_tensor_by_name(\"embeddings:0\")\n","    phase_train_tensor = sess.graph.get_tensor_by_name(\"phase_train:0\")\n","\n","    # Extract embedding\n","    embedding = sess.run(embedding_tensor, feed_dict={input_tensor: image, phase_train_tensor: False})\n","    return embedding\n","\n","# Load and preprocess the image\n","image_path = \"/content/drive/MyDrive/Sample image/Screenshot 2025-02-13 184429.png\"\n","\n","# Load image using OpenCV\n","sample_image = cv2.imread(image_path)\n","\n","# Check if the image loaded successfully\n","if sample_image is None:\n","    raise ValueError(\"Error: Could not load image. Check the file path.\")\n","\n","# Resize to FaceNet input size (160x160)\n","sample_image = cv2.resize(sample_image, (160, 160))\n","\n","# Convert BGR (OpenCV default) to RGB (FaceNet expects RGB)\n","sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n","\n","# Normalize image (0-255 ‚Üí 0-1)\n","sample_image = sample_image.astype(np.float32) / 255.0\n","\n","# Get face embedding\n","embedding = get_face_embedding(sess, sample_image)\n","\n","print(\"Embedding shape:\", embedding.shape)\n","print(\"Face embedding vector:\", embedding)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlcCdMmjypuv","executionInfo":{"status":"ok","timestamp":1739712310847,"user_tz":-60,"elapsed":12215,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"5b3c2c55-8561-4062-8e14-1b0b4fa395f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding shape: (1, 512)\n","Face embedding vector: [[ 2.13637785e-03  5.47381192e-02 -3.75173353e-02 -4.27848995e-02\n","  -9.47598182e-03 -1.63369719e-03 -5.19527085e-02  3.64262052e-02\n","   6.60565123e-02 -5.32447770e-02  6.57847598e-02 -1.80203822e-02\n","   3.00407186e-02 -2.92190909e-02 -9.16443532e-05  4.87631261e-02\n","   2.73835752e-02  3.30392420e-02 -4.72299829e-02 -1.00514442e-02\n","  -2.57615466e-02 -4.60544527e-02  1.07948393e-01  4.29020002e-02\n","  -4.18846644e-02  7.72707090e-02  1.17974905e-02 -1.61479395e-02\n","  -4.41422657e-04  1.39821514e-01  4.36290773e-03 -7.34444149e-03\n","   2.95642354e-02 -1.20688854e-02  1.67980809e-02  2.20519137e-02\n","  -4.41902094e-02  7.55961761e-02  2.78059859e-02 -3.92844975e-02\n","   5.47698059e-04 -4.59438600e-02  1.64092369e-02  3.69936377e-02\n","   1.54784080e-02  1.03229284e-01 -6.86068162e-02 -1.11942835e-01\n","   2.23205946e-02 -5.09044621e-03 -3.80699262e-02 -2.46392353e-03\n","  -9.48980302e-02 -1.46996900e-02 -4.17626537e-02  6.91293739e-03\n","   5.03320433e-02 -2.79907882e-02 -9.19773579e-02 -2.12425850e-02\n","  -5.07739671e-02 -1.01320468e-01  2.78906580e-02 -4.10015434e-02\n","   2.36645471e-02  2.36512329e-02 -5.01106232e-02  3.80223952e-02\n","   1.62587687e-02  2.42399164e-02 -5.18154204e-02 -3.79475206e-02\n","  -1.15537867e-02  2.70113796e-02 -5.83358994e-03 -5.97153744e-03\n","   2.26014592e-02  2.58935560e-02 -1.53210480e-02  1.16524259e-02\n","   9.26097631e-02 -1.19245993e-02 -8.27271491e-02  1.32374112e-02\n","  -2.52345670e-02 -6.16198480e-02  2.01928969e-02  1.11484334e-01\n","  -4.46949787e-02  4.36517932e-02 -5.84968701e-02 -4.13178839e-02\n","  -4.05988060e-02 -5.04052546e-03  6.85433149e-02 -7.22625898e-03\n","  -8.72215629e-02  2.97555029e-02  4.16005142e-02  1.07354932e-02\n","  -2.23774817e-02 -1.34235551e-03 -1.88678782e-02 -6.48968071e-02\n","  -1.12363910e-02 -1.20030381e-02  4.41424921e-02  2.91570164e-02\n","   1.71289372e-04 -2.94505749e-02 -5.01722991e-02  2.27759928e-02\n","   4.22284640e-02 -2.80172601e-02  3.48601677e-02 -5.84325306e-02\n","   3.73563170e-02  3.45553532e-02 -2.41434556e-02  4.57051359e-02\n","   6.45490885e-02  3.43726240e-02 -2.22216249e-02  3.88634950e-02\n","  -6.29983172e-02 -8.65369067e-02  2.09805071e-02 -1.81956519e-03\n","   1.65413935e-02  6.79850020e-03  6.12051785e-02  4.19939496e-02\n","   5.06000631e-02 -3.77911292e-02 -7.45830014e-02 -1.47130964e-02\n","   7.19653368e-02  3.05609945e-02  4.48679365e-02  6.07174933e-02\n","  -5.24542108e-03 -6.47145286e-02  4.88599688e-02 -8.27016011e-02\n","  -5.88790048e-03  1.92189328e-02  7.42371976e-02  1.82971209e-02\n","   3.52596939e-02  6.37950096e-03  1.98961142e-02  3.28136608e-02\n","   5.88101670e-02 -6.39266744e-02 -3.85369211e-02  7.99905360e-02\n","  -1.61179528e-02 -4.69971225e-02  6.97264373e-02 -4.78084870e-02\n","   1.30708739e-02  4.22257744e-02 -3.56237181e-02 -2.67867954e-03\n","  -1.44946184e-02  2.13306975e-02 -3.66639569e-02 -2.84425281e-02\n","  -4.43874635e-02  2.17989320e-03 -5.22345789e-02 -1.16837900e-02\n","   2.36438252e-02  1.22595998e-02 -6.18189661e-05 -2.60401778e-02\n","   5.05401306e-02  2.32170038e-02 -2.91901808e-02  3.40625420e-02\n","   3.13844346e-03  4.18841094e-02 -7.80308992e-03 -1.12164777e-03\n","  -7.36167561e-03 -1.69180427e-02  9.52265710e-02 -8.26316327e-02\n","   2.70650033e-02  5.88564463e-02  2.46526878e-02  6.85362816e-02\n","  -3.69566940e-02  4.73135486e-02  1.48438718e-02  2.07270309e-02\n","  -4.04416956e-03 -5.32274023e-02 -8.99052154e-03 -3.67760472e-02\n","  -3.99148054e-02  2.27857493e-02 -6.39397353e-02 -3.52313779e-02\n","   4.93462086e-02 -7.44700897e-03 -2.33474728e-02  3.36658359e-02\n","   7.26650236e-03  2.70655542e-03  5.94142526e-02 -4.31371434e-03\n","  -1.20988116e-01  7.74610927e-03 -4.17867489e-02  9.38604493e-03\n","   2.76156254e-02 -1.51584214e-02 -3.74137983e-02  8.95271897e-02\n","   1.09909251e-01 -6.19768463e-02 -1.12831686e-02  2.62472443e-02\n","   1.40532255e-02  1.87236955e-03 -3.40532735e-02  4.74302210e-02\n","   1.49970939e-02 -6.37992546e-02 -1.77133922e-02 -2.96238177e-02\n","  -1.45006161e-02 -3.15436274e-02  3.79492976e-02  2.88051763e-03\n","   1.63139515e-02  3.54861617e-02  6.79809153e-02  7.34647140e-02\n","   1.84342936e-02  1.54855633e-02 -5.76522797e-02 -1.11420117e-02\n","   2.81721354e-03 -1.63798511e-03  8.09583068e-02  3.75957158e-03\n","  -2.54720654e-02 -5.49858958e-02  6.66318610e-02 -1.62543617e-02\n","  -2.94765388e-03 -1.05083928e-01  1.59367602e-02  3.25963972e-03\n","  -7.30894357e-02  7.21885785e-02  2.20250394e-02 -2.43018214e-02\n","   1.77363697e-02 -6.36449233e-02  4.69653793e-02  5.11790365e-02\n","  -1.95161235e-02 -8.20350740e-03 -1.65956579e-02  6.06159447e-03\n","  -3.51546183e-02  4.29843664e-02 -2.12290455e-02 -8.17284137e-02\n","   6.87959883e-03 -1.56861532e-03 -8.28913897e-02  1.07200099e-02\n","  -8.64990614e-03 -8.93401951e-02 -5.39780711e-04  8.02872404e-02\n","  -1.98755618e-02 -7.52572715e-02  1.83855575e-02 -4.21712063e-02\n","   2.69259582e-03  5.41913323e-02  6.49605505e-03 -3.01264506e-02\n","   6.13535615e-03  1.29337171e-02 -1.25766052e-02 -3.04613318e-02\n","  -5.09265997e-02 -8.49704724e-03 -2.60327253e-02 -6.46455437e-02\n","  -5.59316911e-02  1.41570643e-02  5.59590524e-03 -4.04085591e-02\n","  -7.86966383e-02  4.72164527e-02 -7.29935244e-02  1.73782855e-02\n","  -9.05202329e-03 -5.19208759e-02  3.75872031e-02  5.81052266e-02\n","  -2.44816002e-02 -6.94009364e-02 -7.56593980e-03  4.06948710e-03\n","   4.58311941e-03  1.95443947e-02 -1.05686123e-02  1.73737668e-02\n","   6.88343346e-02  5.13786562e-02  1.05731003e-02 -2.90332381e-02\n","  -3.99918295e-03  7.04614073e-02  4.50312234e-02  1.77560765e-02\n","  -3.55562754e-02 -1.39297461e-02  2.18240544e-02  4.45530266e-02\n","   3.39645110e-02  1.69763900e-02 -2.74196323e-02  3.40509936e-02\n","  -2.49705892e-02 -3.25030833e-02  5.93726747e-02  8.75025056e-03\n","  -1.35376183e-02 -8.11098814e-02 -5.91426753e-02 -5.45816720e-02\n","  -2.64456850e-02  2.31956206e-02 -2.91377679e-03 -6.02441579e-02\n","  -4.50522341e-02 -3.60055035e-03  4.78853211e-02 -4.93772924e-02\n","  -3.05508245e-02 -1.82546079e-02  7.67032653e-02 -3.75897884e-02\n","  -4.73955572e-02  3.22550423e-02  1.88105293e-02  2.02604420e-02\n","  -5.47665544e-02 -3.27095464e-02  1.84155498e-02 -8.40675011e-02\n","   1.74927292e-03 -1.26968510e-02 -2.48874421e-03  1.56681444e-02\n","   2.86834892e-02  1.69828180e-02 -1.62794106e-02  2.04879940e-02\n","   2.86363007e-04 -1.09057678e-02  1.12596508e-02  6.78445622e-02\n","   1.05403870e-01  5.35312779e-02 -3.28744799e-02  1.83020085e-02\n","   1.82780381e-02  3.10676023e-02 -2.27216575e-02 -1.55785484e-02\n","  -1.55409174e-02 -1.20978713e-01 -3.52655016e-02 -3.35930921e-02\n","   1.25597268e-02 -2.00004149e-02 -3.15761864e-02  3.05553079e-02\n","  -5.52609898e-02 -3.88036892e-02  2.57188943e-03  1.34283543e-01\n","   1.21389134e-02 -1.51970163e-02 -2.74376664e-02 -5.70469070e-03\n","  -5.30869002e-03  2.94124037e-02 -1.84989255e-02  4.67584608e-03\n","   2.43434999e-02  5.57227209e-02  2.02030092e-02  3.09827980e-02\n","  -5.58848940e-02  7.47289881e-02 -4.19310518e-02  3.02563738e-02\n","  -1.78661682e-02 -2.29027644e-02 -5.70572773e-03  1.83872506e-02\n","  -2.93241050e-02 -1.97397582e-02 -8.20434391e-02  5.81307411e-02\n","  -3.51516828e-02 -7.32914582e-02  4.49298434e-02  7.23768473e-02\n","  -7.42309391e-02 -3.70707028e-02 -2.92586051e-02 -8.45092908e-02\n","  -2.54846574e-03 -5.77742942e-02  3.38394456e-02 -6.30733073e-02\n","   5.38969897e-02 -1.94064863e-02  1.43994181e-03 -3.93557502e-03\n","  -3.95508483e-02  2.62977201e-02 -3.94249037e-02  7.47937262e-02\n","  -4.87774573e-02 -1.33665102e-02 -3.29841627e-03 -8.21257159e-02\n","   5.52898757e-02  6.43411186e-03 -9.00273863e-03 -4.03454900e-02\n","   5.27023226e-02 -1.07319802e-02  3.08666416e-02 -1.79824159e-02\n","  -1.88061986e-02 -3.27798538e-02  7.27296667e-03 -7.60142878e-02\n","   3.19736488e-02 -4.77322601e-02  3.21108587e-02 -4.33215173e-03\n","  -4.37235925e-03 -1.14331720e-02 -3.89469638e-02  5.88271990e-02\n","   5.87652661e-02  5.74917719e-03  9.41001996e-02  2.54286397e-02\n","   4.47383597e-02 -6.25015795e-02 -1.35948956e-02  4.73856702e-02\n","   2.69465223e-02  5.05358428e-02  2.42501963e-02 -5.77701926e-02\n","  -7.19771907e-03  1.42762111e-02  4.12096232e-02  1.35867028e-02\n","  -1.03071462e-02 -2.67539416e-02  3.21686529e-02 -4.31346446e-02\n","   6.56486377e-02  1.95852220e-02 -6.38581067e-03  2.96240896e-02\n","   1.53075100e-03 -4.53139432e-02 -7.03866361e-03  1.99350882e-02\n","   6.14087563e-03 -3.97681445e-02  3.02539319e-02 -9.37478170e-02\n","   2.26830356e-02  1.22416221e-01 -6.09224886e-02  2.48542391e-02\n","   7.60974437e-02 -4.62724715e-02 -6.85524149e-03 -1.90027878e-02\n","  -9.17059649e-03 -4.59606573e-02 -1.80106778e-02 -1.34736244e-02\n","   4.15003635e-02  5.75541891e-03 -7.82665387e-02  1.38083145e-01\n","   1.52199389e-02  2.97571644e-02 -1.22419633e-02 -6.95792660e-02]]\n"]}]},{"cell_type":"code","source":["# Ensure images are properly loaded\n","if len(images) > 0:\n","    face_image = images[0]  # Select the first image in the dataset\n","    embedding = get_face_embedding(session, face_image)\n","    print(\"Face Embedding Shape:\", embedding.shape)\n","else:\n","    print(\"No images found in dataset!\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"96aR2TCjvaVN","executionInfo":{"status":"ok","timestamp":1739712316776,"user_tz":-60,"elapsed":5964,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"77ad8c09-44dc-40fc-8a62-05a1a76e0658"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Face Embedding Shape: (1, 512)\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","# Load image from file\n","image_path = \"/content/drive/MyDrive/Sample image/Screenshot 2025-02-13 184429.png\"\n","sample_image = cv2.imread(image_path)\n","\n","# Check if the image loaded successfully\n","if sample_image is None:\n","    raise ValueError(\"Error: Could not load image. Check the file path.\")\n","\n","# Resize to the expected FaceNet input size (160x160)\n","sample_image = cv2.resize(sample_image, (160, 160))\n","\n","# Convert BGR (OpenCV default) to RGB (FaceNet expects RGB)\n","sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n","\n","# Normalize image (0-255 ‚Üí 0-1)\n","sample_image = sample_image / 255.0\n"],"metadata":{"id":"p9MzAvBBxXxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install lz4\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8vTWjmUuTyO","executionInfo":{"status":"ok","timestamp":1739712317391,"user_tz":-60,"elapsed":712,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"ba24fe41-4123-42ae-fdd6-88475ddf48fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: lz4 in /usr/local/lib/python3.11/dist-packages (4.4.3)\n"]}]},{"cell_type":"code","source":["def load_facenet_model(model_path):\n","    if not os.path.exists(model_path):\n","        raise FileNotFoundError(f\"‚ùå Model path '{model_path}' does not exist!\")\n","\n","    print(f\"üì• Loading FaceNet model from: {model_path}\")\n","\n","    with tf.io.gfile.GFile(model_path, \"rb\") as f:\n","        graph_def = tf.compat.v1.GraphDef()\n","        graph_def.ParseFromString(f.read())\n","        tf.import_graph_def(graph_def, name=\"\")\n","\n","    session = tf.compat.v1.Session()\n","    print(\"‚úÖ FaceNet model loaded successfully!\")\n","    return session\n","\n","# üîπ Change this to the actual path of your model\n","model_path = \"/content/drive/MyDrive/facenet_model/20180408-102900/20180408-102900.pb\"\n","session = load_facenet_model(model_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mt7-ZhMV1V9l","executionInfo":{"status":"ok","timestamp":1739712317392,"user_tz":-60,"elapsed":58,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"6194e9d7-c676-4457-aac9-2f247c1e3544"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üì• Loading FaceNet model from: /content/drive/MyDrive/facenet_model/20180408-102900/20180408-102900.pb\n","‚úÖ FaceNet model loaded successfully!\n"]}]},{"cell_type":"code","source":["DATASET_PATH = \"/content/drive/MyDrive/face_recognition_db\"\n","IMAGE_SIZE = (160, 160)  # FaceNet input size\n","\n","def load_images(directory):\n","    images, labels = [], []\n","\n","    if not os.path.exists(directory):\n","        raise FileNotFoundError(f\"‚ùå Dataset path '{directory}' does not exist!\")\n","\n","    print(f\"üìÇ Loading images from {directory}...\")\n","\n","    for filename in os.listdir(directory):\n","        if filename.endswith((\".jpg\", \".png\")):\n","            img_path = os.path.join(directory, filename)\n","            img = cv2.imread(img_path)\n","\n","            if img is None:\n","                print(f\"‚ö†Ô∏è Skipping invalid image: {filename}\")\n","                continue\n","\n","            img = cv2.resize(img, IMAGE_SIZE)\n","            img = img.astype(\"float32\") / 255.0  # Normalize\n","            images.append(img)\n","            labels.append(filename.split(\".\")[0])\n","\n","    images = np.array(images)\n","    print(f\"‚úÖ Loaded {len(images)} images.\")\n","    return images, np.array(labels)\n","\n","images, labels = load_images(DATASET_PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VUsbiQb3x9z","executionInfo":{"status":"ok","timestamp":1739712320446,"user_tz":-60,"elapsed":3078,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"3851c8f6-0c05-466b-bc4d-7dd34c2b4ffe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üìÇ Loading images from /content/drive/MyDrive/face_recognition_db...\n","‚úÖ Loaded 57 images.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","tf.config.run_functions_eagerly(True)\n"],"metadata":{"id":"Mx_Mc-x14mT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import cv2\n","from mtcnn import MTCNN\n","\n","# ... (MTCNN detector initialization and detect_faces function remain the same)\n","\n","images = []\n","\n","# Example: Let's say you have a Keras model and you get the image from the input:\n","# model = ...  # Your Keras model\n","# input_tensor = ... # The input tensor to the model\n","\n","# Convert the input tensor to a NumPy array *immediately*:\n","try:\n","    numpy_image = input_tensor.numpy()  # Convert to NumPy array\n","    images.append(numpy_image)\n","except AttributeError:\n","    print(\"Error: Could not convert TensorFlow Tensor to NumPy array. \"\n","          \"Ensure eager execution is enabled or use tf.function if needed.\")\n","    # Handle the error appropriately, e.g., skip this image.\n","    # return  # If you are in a function, you might want to return here.\n","\n","# OR, if you are getting the image inside a tf.function\n","# @tf.function\n","# def my_function(input_tensor):\n","#     try:\n","#         numpy_image = input_tensor.numpy()\n","#         images.append(numpy_image)\n","#     except AttributeError:\n","#         print(\"Error: Could not convert TensorFlow Tensor to NumPy array. \"\n","#               \"Ensure eager execution is enabled.\")\n","#         return  # Or handle the error as needed\n","\n","\n","\n","if images:\n","    test_face = detect_faces(images[0])  # Now images[0] will be a NumPy array\n","\n","    # ... (rest of your code)\n","if images:\n","    test_face = detect_faces(images[0])\n","\n","    if test_face is not None:\n","        print(\"‚úÖ Face detected and cropped!\")\n","        # ... (display or process test_face)\n","    else:\n","        print(\"Face detection failed for the first image.\")\n","else:\n","    print(\"No images provided.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kivpmn1h8x0N","executionInfo":{"status":"ok","timestamp":1739712320448,"user_tz":-60,"elapsed":41,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"76a5bb2f-b4b2-4589-8e8b-7ce9839ef650"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: Could not convert TensorFlow Tensor to NumPy array. Ensure eager execution is enabled or use tf.function if needed.\n","No images provided.\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","def get_face_embedding(session, image):\n","    \"\"\"\n","    Extract face embeddings from a preprocessed face image using FaceNet.\n","    \"\"\"\n","    if image is None:\n","        print(\"‚ö†Ô∏è Skipping embedding extraction (face not detected).\")\n","        return None\n","\n","    face_pixels = np.expand_dims(image, axis=0)  # Add batch dimension\n","    face_pixels = (face_pixels - 127.5) / 128.0  # Normalize\n","\n","    # Get input/output tensors\n","    try:\n","        input_tensor = session.graph.get_tensor_by_name(\"input:0\")\n","        embeddings_tensor = session.graph.get_tensor_by_name(\"embeddings:0\")\n","        phase_train_tensor = session.graph.get_tensor_by_name(\"phase_train:0\")  # Important: Include phase_train\n","    except KeyError as e:\n","        print(f\"‚ùå Tensor name error: {e}\")\n","        return None\n","\n","    # Run FaceNet model\n","    try:  # Add a try-except block here\n","        embedding = session.run(embeddings_tensor, feed_dict={\n","            input_tensor: face_pixels,\n","            phase_train_tensor: False  # Crucial for consistent results\n","        })\n","        return embedding\n","    except Exception as e: # Catch potential runtime errors during session.run\n","        print(f\"‚ùå Error during session.run: {e}\")\n","        return None\n","\n","\n","\n","# Example usage (assuming you have 'detect_faces' and 'images' defined elsewhere):\n","#  Make sure you have a loaded FaceNet model session\n","\n","# Example using a placeholder for detect_faces and images for testing\n","def detect_faces(image):\n","    # Replace this with your actual face detection logic.\n","    # For testing, return a dummy face if the image exists, otherwise return None.\n","    if image is not None:\n","        return np.random.rand(160, 160, 3) # Dummy face data (adjust dimensions as needed)\n","    return None\n","\n","images = [np.random.rand(200,200,3)] # Example Image data. Replace this with your actual image data.\n","\n","with tf.Graph().as_default(): # This is crucial for loading the model and creating the session\n","    with tf.compat.v1.Session() as session:  # Use tf.compat.v1.Session for compatibility\n","        # Load the FaceNet model (replace with your actual path)\n","        model_path = \"/content/drive/MyDrive/facenet_model/20180408-102900/20180408-102900.pb\" # Example path, replace with your model path.\n","        with tf.compat.v1.gfile.GFile(model_path, \"rb\") as f:\n","            graph_def = tf.compat.v1.GraphDef()\n","            graph_def.ParseFromString(f.read())\n","        tf.import_graph_def(graph_def, name='') # Import the graph\n","\n","        if len(images) > 0:\n","            test_face = detect_faces(images[0])\n","            if test_face is not None:\n","                embedding = get_face_embedding(session, test_face)\n","                if embedding is not None:\n","                    print(f\"‚úÖ Embedding extracted! Shape: {embedding.shape}\")\n","                    print(f\"üîπ First 5 values: {embedding[0][:5]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AOK_rLhCCTa","executionInfo":{"status":"ok","timestamp":1739712453919,"user_tz":-60,"elapsed":9158,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"e9a4975d-7353-401d-c7a6-d565f8f0458f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Embedding extracted! Shape: (1, 512)\n","üîπ First 5 values: [-0.03484491  0.07772427  0.03606033 -0.03689137  0.01590375]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","\n","# ... (Your get_face_embedding and detect_faces functions from previous responses) ...\n","\n","# Example usage (assuming you have 'images' and 'labels' defined)\n","images = [np.random.rand(160, 160, 3), np.random.rand(160, 160, 3), None, np.random.rand(160, 160, 3)] # Example images, replace with your actual data. Include some None images for testing.\n","labels = [\"person1\", \"person2\", \"person3\", \"person4\"] # Example labels\n","\n","with tf.Graph().as_default():\n","    with tf.compat.v1.Session() as session:\n","        model_path = \"/content/drive/MyDrive/facenet_model/20180408-102900/20180408-102900.pb\"  # Replace with your FaceNet model path\n","        with tf.compat.v1.gfile.GFile(model_path, \"rb\") as f:\n","            graph_def = tf.compat.v1.GraphDef()\n","            graph_def.ParseFromString(f.read())\n","        tf.import_graph_def(graph_def, name='')\n","\n","        for i, img in enumerate(images):\n","            print(f\"\\nüîπ Processing Image {i + 1}/{len(images)} - Label: {labels[i]}\")\n","\n","            if img is None: # Corrected condition to check for None directly\n","                print(f\"‚ö†Ô∏è Skipping empty image {i + 1}...\") # Added 1 to i for correct index display\n","                continue\n","\n","            face_image = detect_faces(img)\n","\n","            if face_image is not None:\n","                embedding = get_face_embedding(session, face_image)\n","                if embedding is not None:\n","                    print(f\"‚úÖ Face Embedding Shape: {embedding.shape}\")\n","                    print(f\"üîπ Face Embedding Vector (First 5 values): {embedding[0][:5]}\")\n","                else:\n","                    print(\"‚ö†Ô∏è Embedding extraction failed.\")\n","            else:\n","                print(f\"‚ö†Ô∏è No face detected in image {i + 1}.\") # More informative message\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qbi_c1HZD6T4","executionInfo":{"status":"ok","timestamp":1739712465954,"user_tz":-60,"elapsed":7708,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"faaf0694-9bf6-47e6-f86c-3277993174a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîπ Processing Image 1/4 - Label: person1\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484086  0.07772     0.03605504 -0.03688461  0.01591105]\n","\n","üîπ Processing Image 2/4 - Label: person2\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485064  0.07773434  0.03606208 -0.03687559  0.0159023 ]\n","\n","üîπ Processing Image 3/4 - Label: person3\n","‚ö†Ô∏è Skipping empty image 3...\n","\n","üîπ Processing Image 4/4 - Label: person4\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484985  0.07773289  0.03607474 -0.03687256  0.01589657]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import cv2  # Or your preferred image loading library\n","\n","# ... (Your get_face_embedding and detect_faces functions from previous responses) ...\n","\n","# Load your images and labels (replace with your actual data loading)\n","images = []\n","labels = []\n","\n","# Example using OpenCV to load images from a directory:\n","import os\n","image_directory = \"/content/drive/MyDrive/face_recognition_db\"\n","for filename in os.listdir(image_directory):\n","    if filename.endswith(('.jpg', '.png', '.jpeg')): # Add other image formats as needed\n","        filepath = os.path.join(image_directory, filename)\n","        img = cv2.imread(filepath)  # Use cv2.IMREAD_COLOR for color images\n","        if img is not None:  # Check if image was loaded successfully\n","            images.append(img)\n","            # Extract label from filename (example: person1.jpg -> person1)\n","            label = filename.split(\".\")[0]\n","            labels.append(label)\n","        else:\n","            print(f\"‚ö†Ô∏è Could not load image: {filename}\")\n","\n","\n","if len(images) != 57:  # Verify you have 57 images\n","    print(f\"‚ö†Ô∏è Warning: Expected 57 images, but found {len(images)}. Adjust the image loading if needed.\")\n","elif len(labels) != 57:\n","    print(f\"‚ö†Ô∏è Warning: Expected 57 labels, but found {len(labels)}. Adjust the label loading if needed.\")\n","\n","with tf.Graph().as_default():\n","    with tf.compat.v1.Session() as session:\n","        model_path = \"/content/drive/MyDrive/facenet_model/20180408-102900/20180408-102900.pb\"\n","        with tf.compat.v1.gfile.GFile(model_path, \"rb\") as f:\n","            graph_def = tf.compat.v1.GraphDef()\n","            graph_def.ParseFromString(f.read())\n","        tf.import_graph_def(graph_def, name='')\n","\n","        for i, img in enumerate(images):\n","            print(f\"\\nüîπ Processing Image {i + 1}/{len(images)} - Label: {labels[i]}\")\n","\n","            if img is None:\n","                print(f\"‚ö†Ô∏è Skipping empty image {i + 1}...\")\n","                continue\n","\n","            face_image = detect_faces(img)\n","\n","            if face_image is not None:\n","                embedding = get_face_embedding(session, face_image)\n","                if embedding is not None:\n","                    print(f\"‚úÖ Face Embedding Shape: {embedding.shape}\")\n","                    print(f\"üîπ Face Embedding Vector (First 5 values): {embedding[0][:5]}\")\n","\n","                    # Store the embeddings and labels (important for later use)\n","                    if 'embeddings' not in locals():  # Initialize on the first iteration\n","                        embeddings = []\n","                    embeddings.append(embedding)\n","\n","                    if 'all_labels' not in locals():\n","                        all_labels = []\n","                    all_labels.append(labels[i])\n","                else:\n","                    print(\"‚ö†Ô∏è Embedding extraction failed.\")\n","            else:\n","                print(f\"‚ö†Ô∏è No face detected in image {i + 1}.\")\n","\n","        # After processing all images, you'll have the embeddings and labels:\n","        if 'embeddings' in locals():\n","            embeddings = np.array(embeddings).squeeze() # Convert to NumPy array and remove extra dimension\n","            print(f\"\\n‚úÖ All embeddings extracted! Shape: {embeddings.shape}\")\n","            print(f\"‚úÖ All labels: {all_labels}\")\n","\n","            # Now you can use 'embeddings' and 'all_labels' for further processing\n","            # (e.g., face recognition, clustering, etc.)\n","        else:\n","            print(\"‚ö†Ô∏è No embeddings were generated. Check your images and face detection.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZVGNX1PFBEl","executionInfo":{"status":"ok","timestamp":1739712488239,"user_tz":-60,"elapsed":15854,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"0d07cdb7-992a-44fc-e3f9-453951c24b61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîπ Processing Image 1/57 - Label: abass\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484621  0.0777206   0.0360528  -0.03689415  0.01590258]\n","\n","üîπ Processing Image 2/57 - Label: Abdullah\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484233  0.07772593  0.03607274 -0.03686813  0.01590113]\n","\n","üîπ Processing Image 3/57 - Label: Ashraff\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484479  0.07771087  0.03603813 -0.03688359  0.01589221]\n","\n","üîπ Processing Image 4/57 - Label: donatus\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485067  0.07772826  0.0360713  -0.03687032  0.01590382]\n","\n","üîπ Processing Image 5/57 - Label: drino\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485113  0.07772472  0.03608787 -0.03685237  0.01589864]\n","\n","üîπ Processing Image 6/57 - Label: fagbulu\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484156  0.07771613  0.03606879 -0.03686182  0.01588563]\n","\n","üîπ Processing Image 7/57 - Label: fidelix 2\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484558  0.07771914  0.03605573 -0.0368718   0.0158958 ]\n","\n","üîπ Processing Image 8/57 - Label: fidelix\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484934  0.07772578  0.03605067 -0.03688065  0.01589876]\n","\n","üîπ Processing Image 9/57 - Label: imam Basheer\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.0348568   0.07772648  0.03605727 -0.03686177  0.01590185]\n","\n","üîπ Processing Image 10/57 - Label: IMG-20241021-WA0028\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485398  0.07772505  0.03604692 -0.03687907  0.01589587]\n","\n","üîπ Processing Image 11/57 - Label: IMG-20250207-WA0021\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485218  0.07774728  0.0360858  -0.03686735  0.01590216]\n","\n","üîπ Processing Image 12/57 - Label: IMG-20250208-WA0003\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485536  0.07771511  0.03603696 -0.03687625  0.01589417]\n","\n","üîπ Processing Image 13/57 - Label: IMG-20250208-WA0004\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484573  0.07772199  0.03606474 -0.03688529  0.01590525]\n","\n","üîπ Processing Image 14/57 - Label: IMG-20250208-WA0005\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485332  0.07772624  0.03604404 -0.03687749  0.01589742]\n","\n","üîπ Processing Image 15/57 - Label: IMG-20250208-WA0006\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484657  0.07773331  0.03607921 -0.03686887  0.01590921]\n","\n","üîπ Processing Image 16/57 - Label: IMG-20250208-WA0007\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484698  0.07772643  0.03606872 -0.03687488  0.0158988 ]\n","\n","üîπ Processing Image 17/57 - Label: IMG-20250208-WA0008\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.0348442   0.07773732  0.03606754 -0.03686889  0.01590191]\n","\n","üîπ Processing Image 18/57 - Label: IMG-20250208-WA0009\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485108  0.07773259  0.03607398 -0.0368706   0.01590183]\n","\n","üîπ Processing Image 19/57 - Label: IMG-20250208-WA0010\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485086  0.07773346  0.03605374 -0.03688029  0.01589602]\n","\n","üîπ Processing Image 20/57 - Label: IMG-20250208-WA0011\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484163  0.07771721  0.03606772 -0.03687024  0.01589553]\n","\n","üîπ Processing Image 21/57 - Label: IMG-20250208-WA0012\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484858  0.07773667  0.0360635  -0.03687933  0.01589909]\n","\n","üîπ Processing Image 22/57 - Label: IMG-20250208-WA0013\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484311  0.07772725  0.03606305 -0.03689062  0.01590724]\n","\n","üîπ Processing Image 23/57 - Label: IMG-20250208-WA0014\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484883  0.07773142  0.03606356 -0.03686446  0.01589786]\n","\n","üîπ Processing Image 24/57 - Label: IMG-20250208-WA0015\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484268  0.07772422  0.03608161 -0.03685325  0.01589776]\n","\n","üîπ Processing Image 25/57 - Label: IMG-20250208-WA0016\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484085  0.07772333  0.03607632 -0.03686605  0.01589709]\n","\n","üîπ Processing Image 26/57 - Label: IMG-20250208-WA0017\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485397  0.07774151  0.03605079 -0.0369073   0.01591088]\n","\n","üîπ Processing Image 27/57 - Label: IMG-20250208-WA0018\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485101  0.07772831  0.03606663 -0.03687006  0.01589973]\n","\n","üîπ Processing Image 28/57 - Label: IMG-20250208-WA0019\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485197  0.07774319  0.03607126 -0.03687171  0.01590946]\n","\n","üîπ Processing Image 29/57 - Label: IMG-20250208-WA0020\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484141  0.07773334  0.03605591 -0.03688399  0.01589778]\n","\n","üîπ Processing Image 30/57 - Label: IMG-20250208-WA0021\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485274  0.07772453  0.03606933 -0.03687172  0.01589591]\n","\n","üîπ Processing Image 31/57 - Label: IMG-20250208-WA0022\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484779  0.07773346  0.03606261 -0.03687341  0.01591446]\n","\n","üîπ Processing Image 32/57 - Label: IMG-20250208-WA0023\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485709  0.07773165  0.0360441  -0.03689601  0.01591336]\n","\n","üîπ Processing Image 33/57 - Label: IMG-20250208-WA0024\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484856  0.0777273   0.03607588 -0.03686975  0.01590197]\n","\n","üîπ Processing Image 34/57 - Label: IMG-20250208-WA0025\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485068  0.07772462  0.03604797 -0.0368777   0.01590561]\n","\n","üîπ Processing Image 35/57 - Label: IMG-20250208-WA0026\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485098  0.07773539  0.03605277 -0.03688377  0.01589821]\n","\n","üîπ Processing Image 36/57 - Label: IMG-20250208-WA0032\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485024  0.07773552  0.03607101 -0.03686844  0.01589625]\n","\n","üîπ Processing Image 37/57 - Label: IMG-20250208-WA0033\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485171  0.07772671  0.03604122 -0.03688106  0.01590344]\n","\n","üîπ Processing Image 38/57 - Label: IMG-20250208-WA0034\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.0348474   0.07773155  0.03608478 -0.03685894  0.01590087]\n","\n","üîπ Processing Image 39/57 - Label: IMG-20250208-WA0035\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484824  0.07771942  0.036057   -0.03688802  0.01589345]\n","\n","üîπ Processing Image 40/57 - Label: IMG-20250208-WA0036\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485603  0.07771887  0.03604673 -0.03688278  0.01589428]\n","\n","üîπ Processing Image 41/57 - Label: IMG-20250208-WA0037\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484992  0.07773086  0.03605726 -0.03686702  0.01590342]\n","\n","üîπ Processing Image 42/57 - Label: IMG-20250208-WA0038\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485044  0.07774013  0.03607694 -0.03687631  0.01589759]\n","\n","üîπ Processing Image 43/57 - Label: IMG-20250208-WA0039\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484567  0.0777243   0.03605457 -0.03687621  0.0158999 ]\n","\n","üîπ Processing Image 44/57 - Label: IMG-20250208-WA0040\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484612  0.07772924  0.03605103 -0.03688448  0.01590659]\n","\n","üîπ Processing Image 45/57 - Label: IMG-20250208-WA0041\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485537  0.07772379  0.03606662 -0.03686659  0.01589902]\n","\n","üîπ Processing Image 46/57 - Label: IMG-20250208-WA0042\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485259  0.07773132  0.03605307 -0.0368709   0.01589472]\n","\n","üîπ Processing Image 47/57 - Label: jakusi\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485874  0.07771607  0.0360493  -0.03686417  0.01589531]\n","\n","üîπ Processing Image 48/57 - Label: jamiu\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484716  0.07773727  0.03606208 -0.03687483  0.0159045 ]\n","\n","üîπ Processing Image 49/57 - Label: mickey\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485885  0.0777249   0.03606118 -0.03686827  0.0158978 ]\n","\n","üîπ Processing Image 50/57 - Label: mubarak\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485345  0.0777348   0.03607872 -0.03686774  0.015913  ]\n","\n","üîπ Processing Image 51/57 - Label: nurse\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485139  0.07774144  0.03607493 -0.03687098  0.0159071 ]\n","\n","üîπ Processing Image 52/57 - Label: okiki\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485081  0.07772455  0.03605722 -0.03687978  0.01589667]\n","\n","üîπ Processing Image 53/57 - Label: olacash\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484876  0.07771916  0.03605065 -0.03687721  0.01590074]\n","\n","üîπ Processing Image 54/57 - Label: omo pastor\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03483926  0.07773048  0.03605659 -0.03688318  0.01590171]\n","\n","üîπ Processing Image 55/57 - Label: Timon\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484562  0.07774191  0.03605855 -0.03689181  0.01590935]\n","\n","üîπ Processing Image 56/57 - Label: tory\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03484924  0.07772165  0.03606768 -0.03686081  0.01590291]\n","\n","üîπ Processing Image 57/57 - Label: unique\n","‚úÖ Face Embedding Shape: (1, 512)\n","üîπ Face Embedding Vector (First 5 values): [-0.03485648  0.077728    0.03606224 -0.03686639  0.01589223]\n","\n","‚úÖ All embeddings extracted! Shape: (57, 512)\n","‚úÖ All labels: ['abass', 'Abdullah', 'Ashraff', 'donatus', 'drino', 'fagbulu', 'fidelix 2', 'fidelix', 'imam Basheer', 'IMG-20241021-WA0028', 'IMG-20250207-WA0021', 'IMG-20250208-WA0003', 'IMG-20250208-WA0004', 'IMG-20250208-WA0005', 'IMG-20250208-WA0006', 'IMG-20250208-WA0007', 'IMG-20250208-WA0008', 'IMG-20250208-WA0009', 'IMG-20250208-WA0010', 'IMG-20250208-WA0011', 'IMG-20250208-WA0012', 'IMG-20250208-WA0013', 'IMG-20250208-WA0014', 'IMG-20250208-WA0015', 'IMG-20250208-WA0016', 'IMG-20250208-WA0017', 'IMG-20250208-WA0018', 'IMG-20250208-WA0019', 'IMG-20250208-WA0020', 'IMG-20250208-WA0021', 'IMG-20250208-WA0022', 'IMG-20250208-WA0023', 'IMG-20250208-WA0024', 'IMG-20250208-WA0025', 'IMG-20250208-WA0026', 'IMG-20250208-WA0032', 'IMG-20250208-WA0033', 'IMG-20250208-WA0034', 'IMG-20250208-WA0035', 'IMG-20250208-WA0036', 'IMG-20250208-WA0037', 'IMG-20250208-WA0038', 'IMG-20250208-WA0039', 'IMG-20250208-WA0040', 'IMG-20250208-WA0041', 'IMG-20250208-WA0042', 'jakusi', 'jamiu', 'mickey', 'mubarak', 'nurse', 'okiki', 'olacash', 'omo pastor', 'Timon', 'tory', 'unique']\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from scipy.spatial.distance import cosine, euclidean\n","import numpy as np\n","from scipy.spatial.distance import cosine, euclidean\n","\n","# Example stored embeddings (from your database)\n","stored_embedding = np.array([-0.03484691,  0.07771436,  0.03605397, -0.03688477,  0.01589229])\n","\n","# New face embedding (from a user input image)\n","new_embedding = np.array([2.13637785e-03,  5.47381192e-02, -3.75173353e-02, -4.27848995e-02,\n","  -9.47598182e-03])\n","\n","# Compute similarity scores\n","cosine_sim = 1 - cosine(stored_embedding, new_embedding)  # Higher = more similar\n","euclidean_dist = euclidean(stored_embedding, new_embedding)  # Lower = more similar\n","\n","import numpy as np\n","from scipy.spatial.distance import cosine, euclidean\n","import numpy as np\n","from scipy.spatial.distance import cosine, euclidean\n","\n","# Example stored embeddings (from your database)\n","stored_embedding = np.array([-0.03484691,  0.07771436,  0.03605397, -0.03688477,  0.01589229])\n","\n","# New face embedding (from a user input image)\n","new_embedding = np.array([2.13637785e-03,  5.47381192e-02, -3.75173353e-02, -4.27848995e-02,\n","  -9.47598182e-03])\n","\n","# Compute similarity scores\n","cosine_sim = 1 - cosine(stored_embedding, new_embedding)  # Higher = more similar\n","euclidean_dist = euclidean(stored_embedding, new_embedding)  # Lower = more similar\n","\n","print(f\"Cosine Similarity: {cosine_sim}\")\n","print(f\"Euclidean Distance: {euclidean_dist}\")\n","#new_embedding = np.array((First 5 values): [2.13637785e-03  5.47381192e-02 -3.75173353e-02 -4.27848995e-02\n","#  -9.47598182e-03])  #This line was causing the syntax error.  It's commented out now.\n","\n","# Compute similarity scores\n","#cosine_sim = 1 - cosine(stored_embedding, new_embedding)  # Higher = more similar\n","#euclidean_dist = euclidean(stored_embedding, new_embedding)  # Lower = more similar\n","\n","#print(f\"Cosine Similarity: {cosine_sim}\")\n","#print(f\"Euclidean Distance: {euclidean_dist}\")\n","# Compute similarity scores\n","cosine_sim = 1 - cosine(stored_embedding, new_embedding)  # Higher = more similar\n","euclidean_dist = euclidean(stored_embedding, new_embedding)  # Lower = more similar\n","\n","print(f\"Cosine Similarity: {cosine_sim}\")\n","print(f\"Euclidean Distance: {euclidean_dist}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"drLq8riRFybN","executionInfo":{"status":"ok","timestamp":1739712537515,"user_tz":-60,"elapsed":630,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"70788afb-eb99-4619-9c8b-4fed19d24ff6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine Similarity: 0.530377123150559\n","Euclidean Distance: 0.08936872455334431\n","Cosine Similarity: 0.530377123150559\n","Euclidean Distance: 0.08936872455334431\n"]}]},{"cell_type":"code","source":["threshold = 0.8  # Example for Euclidean Distance\n","\n","if euclidean_dist < threshold:\n","    print(\"Match Found ‚úÖ\")\n","else:\n","    print(\"No Match ‚ùå\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LritYP9ahNB","executionInfo":{"status":"ok","timestamp":1739712558442,"user_tz":-60,"elapsed":718,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"efa0ac05-c947-4157-f60f-b98cd2c73c95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Match Found ‚úÖ\n"]}]},{"cell_type":"code","source":["def is_match(embedding1, embedding2, threshold=0.8):\n","    return euclidean(embedding1, embedding2) < threshold\n","\n","# Test with sample embeddings\n","print(is_match(stored_embedding, new_embedding))  # Should return True/False\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CFYeGpLnalXz","executionInfo":{"status":"ok","timestamp":1739712569300,"user_tz":-60,"elapsed":1214,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"cf20ae94-00ef-4d7e-b774-65d03914ecc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["import json\n","\n","# Example: embeddings stored in a structured dictionary\n","embeddings = [\n","    {\n","        \"image\": f\"Image {i+1}/57\",\n","        \"label\": label,\n","        \"embedding_shape\": (1, 512),\n","        \"embedding_vector\": vector[:5]  # First 5 values only\n","    }\n","    for i, (label, vector) in enumerate([\n","        (\"abass\", [-0.03484719, 0.07771491, 0.03604504, -0.03688465, 0.01589953]),\n","        (\"Abdullah\", [-0.03485018, 0.07772791, 0.03607094, -0.03686009, 0.01588774]),\n","        (\"Ashraff\", [-0.03484937, 0.07772201, 0.03606177, -0.03687428, 0.01590424]),\n","        (\"donatus\", [-0.03484838, 0.07774416, 0.03608835, -0.03685204, 0.01589765]),\n","        (\"drino\", [-0.03483947, 0.07773511, 0.0360751, -0.03687627, 0.01590466]),\n","        (\"fagbulu\", [-0.0348538, 0.07772313, 0.03604557, -0.03687536, 0.01589679]),\n","        (\"fidelix 2\", [-0.03484766, 0.07773087, 0.03607897, -0.03685197, 0.01589281]),\n","        (\"fidelix\", [-0.03485319, 0.07772647, 0.036068, -0.03686265, 0.01589356]),\n","        (\"imam Basheer\", [-0.03484721, 0.07772658, 0.03607843, -0.03686048, 0.01589545]),\n","        *[(f\"IMG-20241021-WA{str(i).zfill(4)}\", [-0.03485, 0.07773, 0.03607, -0.03687, 0.01590]) for i in range(3, 43)],\n","        (\"John\", [-0.03484587, 0.07773256, 0.03608145, -0.03686423, 0.01589974]),\n","        (\"joseph\", [-0.03484892, 0.07773654, 0.03606778, -0.03686814, 0.01589147]),\n","        (\"kareem\", [-0.03484932, 0.07773923, 0.03607259, -0.03685912, 0.01589982]),\n","        (\"kenny\", [-0.03485156, 0.07774189, 0.03608012, -0.03687094, 0.01590212]),\n","        (\"Lekan\", [-0.03484745, 0.07772898, 0.03606531, -0.03687476, 0.01589756]),\n","        (\"mohammed\", [-0.03484682, 0.07773527, 0.03607815, -0.03686542, 0.01590234]),\n","        (\"musa\", [-0.03484998, 0.07772412, 0.03607189, -0.03687489, 0.01589478]),\n","        (\"Nnamdi\", [-0.03484872, 0.07773743, 0.03606845, -0.03687312, 0.01589942]),\n","        (\"obi\", [-0.03484923, 0.07772954, 0.03607721, -0.03685967, 0.01590123]),\n","        (\"paul\", [-0.03485091, 0.07773348, 0.03606975, -0.03687215, 0.01589865]),\n","        (\"Philip\", [-0.03484853, 0.07773829, 0.03607318, -0.03687021, 0.01590267]),\n","        (\"Rasheed\", [-0.03484974, 0.07773012, 0.03607656, -0.03686147, 0.01589534]),\n","        (\"samuel\", [-0.03484798, 0.07773942, 0.03606584, -0.03687038, 0.01590012]),\n","        (\"sani\", [-0.03485126, 0.07773154, 0.03607931, -0.03686875, 0.01589921]),\n","        (\"sarah\", [-0.03484879, 0.07774031, 0.03607486, -0.03687249, 0.01589874]),\n","        (\"tunde\", [-0.03485065, 0.07773398, 0.03607215, -0.03686945, 0.01589712]),\n","        (\"Umar\", [-0.03484947, 0.07773625, 0.03606743, -0.03687367, 0.01590245]),\n","        (\"victor\", [-0.03485089, 0.07772871, 0.03607087, -0.03687092, 0.01590132]),\n","        (\"Yakubu\", [-0.03484721, 0.07773519, 0.03607998, -0.03686847, 0.01590064]),\n","        (\"Zainab\", [-0.03484954, 0.07773187, 0.03607421, -0.03686951, 0.01589989])\n","    ])\n","]\n","\n","# Save to JSON file\n","with open(\"embeddings.json\", \"w\") as json_file:\n","    json.dump(embeddings, json_file, indent=4)\n","\n","# Print sample output\n","print(json.dumps(embeddings[:5], indent=4))  # Display first 5 for verification\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRtfDqKxjdis","executionInfo":{"status":"ok","timestamp":1739712577844,"user_tz":-60,"elapsed":664,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"fb0bebb6-7454-4bc1-c2cf-d2bf93d573a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[\n","    {\n","        \"image\": \"Image 1/57\",\n","        \"label\": \"abass\",\n","        \"embedding_shape\": [\n","            1,\n","            512\n","        ],\n","        \"embedding_vector\": [\n","            -0.03484719,\n","            0.07771491,\n","            0.03604504,\n","            -0.03688465,\n","            0.01589953\n","        ]\n","    },\n","    {\n","        \"image\": \"Image 2/57\",\n","        \"label\": \"Abdullah\",\n","        \"embedding_shape\": [\n","            1,\n","            512\n","        ],\n","        \"embedding_vector\": [\n","            -0.03485018,\n","            0.07772791,\n","            0.03607094,\n","            -0.03686009,\n","            0.01588774\n","        ]\n","    },\n","    {\n","        \"image\": \"Image 3/57\",\n","        \"label\": \"Ashraff\",\n","        \"embedding_shape\": [\n","            1,\n","            512\n","        ],\n","        \"embedding_vector\": [\n","            -0.03484937,\n","            0.07772201,\n","            0.03606177,\n","            -0.03687428,\n","            0.01590424\n","        ]\n","    },\n","    {\n","        \"image\": \"Image 4/57\",\n","        \"label\": \"donatus\",\n","        \"embedding_shape\": [\n","            1,\n","            512\n","        ],\n","        \"embedding_vector\": [\n","            -0.03484838,\n","            0.07774416,\n","            0.03608835,\n","            -0.03685204,\n","            0.01589765\n","        ]\n","    },\n","    {\n","        \"image\": \"Image 5/57\",\n","        \"label\": \"drino\",\n","        \"embedding_shape\": [\n","            1,\n","            512\n","        ],\n","        \"embedding_vector\": [\n","            -0.03483947,\n","            0.07773511,\n","            0.0360751,\n","            -0.03687627,\n","            0.01590466\n","        ]\n","    }\n","]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import json\n","import cv2\n","\n","\n","# Path to stored embeddings\n","SAVE_PATH = \"embeddings.json\"\n","\n","# Load stored embeddings\n","def load_embeddings():\n","    with open(SAVE_PATH, \"r\") as file:\n","        return json.load(file)\n","\n","# Load face database\n","face_db = load_embeddings()\n"],"metadata":{"id":"Kvbk4kQ1rwGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def recognize_face(face_embedding):\n","    best_match = None\n","    best_distance = float(\"inf\")\n","    threshold = 0.8  # Set a strict threshold\n","\n","    for name, stored_embedding in face_db.items():\n","        stored_embedding = np.array(stored_embedding)\n","        distance = np.linalg.norm(face_embedding - stored_embedding)\n","\n","        if distance < best_distance and distance < threshold:\n","            best_match = name\n","            best_distance = distance\n","\n","    return best_match if best_match else \"Face Not Found\"\n"],"metadata":{"id":"aKPYPeKRvF6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Open webcam\n","cap = cv2.VideoCapture(0)\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    face_locations = face_recognition.face_locations(rgb_frame)\n","    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n","\n","    for (top, right, bottom, left), face_embedding in zip(face_locations, face_encodings):\n","        name = recognize_face(face_embedding)\n","\n","        color = (0, 255, 0) if name != \"Face Not Found\" else (0, 0, 255)\n","\n","        # Draw bounding box and label\n","        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n","        cv2.putText(frame, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n","\n","    cv2.imshow(\"Face Recognition\", frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"],"metadata":{"id":"-jB16GVQvNRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","\n","cap = cv2.VideoCapture(2)\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        print(\"‚ö†Ô∏è Error: Webcam not detected or already in use!\")\n","        break\n","\n","    cv2.imshow(\"Webcam Test\", frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yoz5CoSw2pXP","executionInfo":{"status":"ok","timestamp":1739712647494,"user_tz":-60,"elapsed":617,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"35d9c05a-6a7e-4a07-d8ef-0a091023f7d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚ö†Ô∏è Error: Webcam not detected or already in use!\n"]}]},{"cell_type":"code","source":["import cv2\n","\n","for i in range(5):  # Check up to 5 possible cameras\n","    cap = cv2.VideoCapture(i)\n","    if cap.isOpened():\n","        print(f\"‚úÖ Webcam found at index {i}\")\n","        cap.release()\n"],"metadata":{"id":"fYBSVDjh3Nl7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","\n","cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # Windows DirectShow\n","\n","if not cap.isOpened():\n","    print(\"‚ö†Ô∏è No webcam detected!\")\n","else:\n","    print(\"‚úÖ Webcam detected!\")\n","    cap.release()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwpnXM8j4Ofj","executionInfo":{"status":"ok","timestamp":1739712667725,"user_tz":-60,"elapsed":1833,"user":{"displayName":"Oluwafunke Georgeline Adefoluke","userId":"04294589572287860396"}},"outputId":"dfa3e3f8-b9d3-4e7d-df77-443fc1728527"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚ö†Ô∏è No webcam detected!\n"]}]},{"cell_type":"code","source":["cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # Use DirectShow backend\n"],"metadata":{"id":"-u8G7iFa5aA6"},"execution_count":null,"outputs":[]}]}